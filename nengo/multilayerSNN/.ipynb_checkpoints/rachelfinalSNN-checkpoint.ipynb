{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points of difference:\n",
    "- When threshold is initialized as ones/Vt (like in matlab) instead of zeros, there are no waves, only when initialized as zeros (unlike matlab code).\n",
    "\n",
    "- Learning rule (solving weight matrix dynamical system) in Matlab takes input as spiked and output y variable of the layer. Instead here, we use Signals of presynaptic and postsynaptic activity from Nengo.\n",
    "\n",
    "- Did not include line from matlab code for solving W1 matrix dynamical system : W1(:,fireL) = 10*(W1(:,fireL))./range(W1(:,fireL),1);\n",
    "\n",
    "- Nengo often uuses linear filters on activities of the pre-synaptic population and/or the post-synaptic population for their learning rules. We use none here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Learning Rule\n",
    "# Subclass of LearningRuleType\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import nengo\n",
    "from nengo.rc import rc\n",
    "from nengo.params import Parameter, NumberParam, NdarrayParam, Default\n",
    "from nengo.neurons import settled_firingrate, LIFRate \n",
    "from nengo.builder.operator import Operator, Copy\n",
    "from nengo.builder import Builder\n",
    "from nengo.builder.signal import Signal\n",
    "from nengo.synapses import Lowpass, SynapseParam\n",
    "from nengo.builder.learning_rules import get_pre_ens, get_post_ens, build_or_passthrough\n",
    "\n",
    "\n",
    "class STDP(nengo.learning_rules.LearningRuleType):\n",
    "    \"\"\"STDP Hebbian learning rule.\n",
    "    Modifies connection weights according to the Hebbian STDP rule.\n",
    "    Notes\n",
    "    -----\n",
    "    The rule is dependent on pre and post neural activities,\n",
    "    not decoded values, and so is not affected by changes in the\n",
    "    size of pre and post ensembles. However, if you are decoding from\n",
    "    the post ensemble, the Oja rule will have an increased effect on\n",
    "    larger post ensembles because more connection weights are changing.\n",
    "    In these cases, it may be advantageous to scale the learning rate\n",
    "    on the rule by ``1 / post.n_neurons``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float, optional\n",
    "        A scalar indicating the rate at which weights will be adjusted.\n",
    "    pre_synapse : `.Synapse`, optional\n",
    "        Synapse model used to filter the pre-synaptic activities.\n",
    "    post_synapse : `.Synapse`, optional\n",
    "        Synapse model used to filter the post-synaptic activities.\n",
    "        If None, ``post_synapse`` will be the same as ``pre_synapse``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    learning_rate : float\n",
    "    A scalar indicating the rate at which weights will be adjusted.\n",
    "    post_synapse : `.Synapse`\n",
    "        Synapse model used to filter the post-synaptic activities.\n",
    "    pre_synapse : `.Synapse`\n",
    "        Synapse model used to filter the pre-synaptic activities.\n",
    "    \"\"\"\n",
    "\n",
    "    modifies = \"weights\"\n",
    "    probeable = (\"pre_filtered\", \"post_filtered\", \"delta\")\n",
    "\n",
    "    learning_rate = NumberParam(\"learning_rate\", low=0, readonly=True, default=.0001)\n",
    "    pre_synapse = SynapseParam(\"pre_synapse\", default=None, readonly=True)\n",
    "    post_synapse = SynapseParam(\"post_synapse\", default=None, readonly=True)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=Default,\n",
    "        pre_synapse=Default,\n",
    "        post_synapse=Default,\n",
    "    ):\n",
    "        super().__init__(learning_rate, size_in=0)\n",
    "\n",
    "        self.pre_synapse = pre_synapse\n",
    "        self.post_synapse = (\n",
    "            self.pre_synapse if post_synapse is Default else post_synapse\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _argreprs(self):\n",
    "        return _remove_default_post_synapse(super()._argreprs, self.pre_synapse)\n",
    "    \n",
    "\n",
    "    \n",
    "# Simulator Operator that implements learning rule\n",
    "\n",
    "class SimSTDP(Operator):\n",
    "    r\"\"\"Calculate connection weight change according to the STDP Hebbian rule.\n",
    "    Implements the Hebbian learning rule of the form\n",
    "    .. math:: \\Delta \\omega_{ij} = \\kappa (a_i a_j)\n",
    "    where\n",
    "    * :math:`\\kappa` is a scalar learning rate,\n",
    "    * :math:`a_i` is the activity of a presynaptic neuron,\n",
    "    * :math:`a_j` is the activity of a postsynaptic neuron,\n",
    "    Parameters\n",
    "    ----------\n",
    "    pre_filtered : Signal\n",
    "        The presynaptic activity, :math:`a_i`.\n",
    "    post_filtered : Signal\n",
    "        The postsynaptic activity, :math:`a_j`.\n",
    "    delta : Signal\n",
    "        The synaptic weight change to be applied, :math:`\\Delta \\omega_{ij}`.\n",
    "    learning_rate : float\n",
    "        The scalar learning rate, :math:`\\kappa`.\n",
    "    tag : str, optional\n",
    "        A label associated with the operator, for debugging purposes.\n",
    "    Attributes\n",
    "    ----------\n",
    "    delta : Signal\n",
    "        The synaptic weight change to be applied, :math:`\\Delta \\omega_{ij}`.\n",
    "    learning_rate : float\n",
    "        The scalar learning rate, :math:`\\kappa`.\n",
    "    post_filtered : Signal\n",
    "        The postsynaptic activity, :math:`a_j`.\n",
    "    pre_filtered : Signal\n",
    "        The presynaptic activity, :math:`a_i`.\n",
    "    tag : str or None\n",
    "        A label associated with the operator, for debugging purposes.\n",
    "    Notes\n",
    "    -----\n",
    "    1. sets ``[]``\n",
    "    2. incs ``[]``\n",
    "    3. reads ``[pre_filtered, post_filtered, weights]``\n",
    "    4. updates ``[delta]``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, pre_filtered, post_filtered, delta, learning_rate, tag=None\n",
    "    ):\n",
    "        super().__init__(tag=tag)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.sets = []\n",
    "        self.incs = []\n",
    "        self.reads = [pre_filtered, post_filtered]\n",
    "        self.updates = [delta]\n",
    "\n",
    "    @property\n",
    "    def delta(self):\n",
    "        return self.updates[0]\n",
    "\n",
    "    @property\n",
    "    def pre_filtered(self):\n",
    "        return self.reads[0]\n",
    "\n",
    "    @property\n",
    "    def post_filtered(self):\n",
    "        return self.reads[1]\n",
    "\n",
    "    @property\n",
    "    def _descstr(self):\n",
    "        return \"pre=%s, post=%s -> %s\" % (\n",
    "            self.pre_filtered,\n",
    "            self.post_filtered,\n",
    "            self.delta,\n",
    "        )\n",
    "\n",
    "    def make_step(self, signals, dt, rng):\n",
    "        pre_filtered = signals[self.pre_filtered]\n",
    "        post_filtered = signals[self.post_filtered]\n",
    "        delta = signals[self.delta]\n",
    "        alpha = self.learning_rate * dt\n",
    "\n",
    "        def step_simSTDP():\n",
    "            # perform update\n",
    "            delta[...] += alpha* np.outer(post_filtered, pre_filtered)\n",
    "#             print(delta[delta!=0])\n",
    "\n",
    "        return step_simSTDP\n",
    "    \n",
    "    \n",
    "# Build function for new learning type\n",
    "\n",
    "@Builder.register(STDP)\n",
    "def build_STDP(model, STDP, rule):\n",
    "    \"\"\"Builds a `.STDP` object into a model.\n",
    "    Calls synapse build functions to filter the pre and post activities,\n",
    "    and adds a `.SimSTDP` operator to the model to calculate the delta.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Model\n",
    "        The model to build into.\n",
    "    STDP : STDP\n",
    "        Learning rule type to build.\n",
    "    rule : LearningRule\n",
    "        The learning rule object corresponding to the neuron type.\n",
    "    Notes\n",
    "    -----\n",
    "    Does not modify ``model.params[]`` and can therefore be called\n",
    "    more than once with the same `.STDP` instance.\n",
    "    \"\"\"\n",
    "\n",
    "    conn = rule.connection\n",
    "    pre_activities = model.sig[get_pre_ens(conn).neurons][\"out\"]\n",
    "    post_activities = model.sig[get_post_ens(conn).neurons][\"out\"]\n",
    "    pre_filtered = build_or_passthrough(model, STDP.pre_synapse, pre_activities)\n",
    "    post_filtered = build_or_passthrough(model, STDP.post_synapse, post_activities)\n",
    "\n",
    "    model.add_op(\n",
    "        SimSTDP(\n",
    "            pre_filtered,\n",
    "            post_filtered,\n",
    "            model.sig[rule][\"delta\"],\n",
    "            learning_rate=STDP.learning_rate,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # expose these for probes\n",
    "    model.sig[rule][\"pre_filtered\"] = pre_filtered\n",
    "    model.sig[rule][\"post_filtered\"] = post_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom LIF Neuron Type\n",
    "# Frontend Neurons subclass\n",
    "class CustomLIF(nengo.neurons.NeuronType):\n",
    "    \"\"\"Spiking version of the leaky integrate-and-fire (LIF) neuron model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tau_rc : float\n",
    "        Membrane RC time constant, in seconds. Affects how quickly the membrane\n",
    "        voltage decays to zero in the absence of input (larger = slower decay).\n",
    "    min_voltage : float\n",
    "        Minimum value for the membrane voltage. If ``-np.inf``, the voltage\n",
    "        is never clipped.\n",
    "    num : int\n",
    "        Number of neurons in the layer.\n",
    "    S : ndarray\n",
    "        Intra-layer adjacency matrix.\n",
    "    tau_th : int\n",
    "    th_plus : int\n",
    "        Rate by which threshold increases whenever a neuron is spiking\n",
    "    v_th : int\n",
    "        Default threshold\n",
    "    v_reset : ndarray\n",
    "    nx : ndarray\n",
    "    fnoise : ndarray\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    probeable = (\"spikes\", \"voltage\", \"threshold\")\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        S, \n",
    "        num, \n",
    "        tau_rc, \n",
    "        tau_th,\n",
    "        th_plus,\n",
    "        v_th,\n",
    "        v_reset,\n",
    "        nx,\n",
    "        fnoise,\n",
    "        thresh,\n",
    "        min_voltage=0, \n",
    "        step=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tau_rc=tau_rc\n",
    "        self.min_voltage=min_voltage\n",
    "        self.num=num\n",
    "        self.S=S\n",
    "        self.nx=nx\n",
    "        self.tau_th=tau_th\n",
    "        self.th_plus=th_plus\n",
    "        self.v_th=v_th\n",
    "        self.v_reset=v_reset\n",
    "        self.fnoise=fnoise\n",
    "        self.thresh=thresh\n",
    "        self.step=step\n",
    "\n",
    "    def gain_bias(self, max_rates, intercepts):\n",
    "        \"\"\"Set gain and bias.\"\"\"\n",
    "        gain = np.ones((self.num,))\n",
    "        bias = np.zeros((self.num,))\n",
    "        return gain, bias\n",
    "\n",
    "    def rates(self, x, gain, bias):\n",
    "        \"\"\"Estimates steady-state firing rate given gain and bias.\"\"\"\n",
    "        J = self.current(x, gain, bias)\n",
    "        voltage = np.zeros_like(J)\n",
    "        threshold = np.zeros_like(J)\n",
    "        return settled_firingrate(\n",
    "            self.step_math, J, [voltage, threshold], settle_time=0.001, sim_time=1.0\n",
    "        )\n",
    "\n",
    "    def step_math(self, dt, J, spiked, voltage, threshold): \n",
    "        # determine which neurons spiked (set spiked voltages to v_reset)\n",
    "\n",
    "        spiked_mask = voltage > threshold\n",
    "        voltage[spiked_mask] = self.v_reset[spiked_mask]\n",
    "        spiked[:] = spiked_mask / dt\n",
    "\n",
    "        H = spiked * dt\n",
    "        if self.num == nR:\n",
    "            U = np.matmul(H, self.S)\n",
    "            eta = self.fnoise[:,self.step]\n",
    "            dV = (1/self.tau_rc) * (-1*voltage + U + eta)\n",
    "\n",
    "            voltage[:] += dV * dt * 100\n",
    "\n",
    "        elif self.num == nL:\n",
    "            # Inputs to LGN: Weights -> Activation function -> Competition rule\n",
    "            x = np.maximum(J - self.thresh, 0) \n",
    "            win, maxInd = np.max(x), np.argmax(x) #maxk doesn't exist in python\n",
    "            \n",
    "            # LGN competition rule\n",
    "            x[x < win] = 0 # only allow winner to participate (make all other entries 0)\n",
    "            U = np.matmul(H, self.S) + x\n",
    "            dV = 1/self.tau_rc*(-1*voltage + U)\n",
    "            voltage[:] += dV * dt * 100\n",
    "\n",
    "            # update thresh\n",
    "            self.thresh += 0.01 * x * dt\n",
    "            \n",
    "        # step threshold voltage (theta)\n",
    "        dTh = (1/self.tau_th)*(self.v_th-threshold)*(1-H)+self.th_plus*H\n",
    "        threshold[:] += dTh * dt * 100\n",
    "        \n",
    "        self.step += 1\n",
    "        \n",
    "        \n",
    "# Backend Operator subclass\n",
    "class SimCustomLIF(Operator):\n",
    "    \"\"\"Set a neuron model output for the given input current.\n",
    "\n",
    "    Implements ``neurons.step_math(dt, J, output, *states)``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neurons : NeuronType\n",
    "        The `.NeuronType`, which defines a ``step_math`` function.\n",
    "    J : Signal\n",
    "        The input current.\n",
    "    output : Signal\n",
    "        The neuron output signal that will be set.\n",
    "    states : list, optional\n",
    "        A list of additional neuron state signals set by ``step_math``.\n",
    "    tag : str, optional\n",
    "        A label associated with the operator, for debugging purposes.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    J : Signal\n",
    "        The input current.\n",
    "    neurons : NeuronType\n",
    "        The `.NeuronType`, which defines a ``step_math`` function.\n",
    "    output : Signal\n",
    "        The neuron output signal that will be set.\n",
    "    states : list\n",
    "        A list of additional neuron state signals set by ``step_math``.\n",
    "    tag : str or None\n",
    "        A label associated with the operator, for debugging purposes.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    1. sets ``[output] + states``\n",
    "    2. incs ``[]``\n",
    "    3. reads ``[J]``\n",
    "    4. updates ``[]``\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, neurons, J, output, states=None, tag=None):\n",
    "        super().__init__(tag=tag)\n",
    "        self.neurons = neurons\n",
    "\n",
    "        self.sets = [output] + ([] if states is None else states)\n",
    "        self.incs = []\n",
    "        self.reads = [J]\n",
    "        self.updates = []\n",
    "\n",
    "    @property\n",
    "    def J(self):\n",
    "        return self.reads[0]\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        return self.sets[0]\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        return self.sets[1:]\n",
    "\n",
    "    def _descstr(self):\n",
    "        return \"%s, %s, %s\" % (self.neurons, self.J, self.output)\n",
    "\n",
    "    def make_step(self, signals, dt, rng):\n",
    "        J = signals[self.J]\n",
    "        output = signals[self.output]\n",
    "        states = [signals[state] for state in self.states]\n",
    "\n",
    "        def step_simcustomlif():\n",
    "            self.neurons.step_math(dt, J, output, *states)\n",
    "\n",
    "        return step_simcustomlif\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Build function\n",
    "@Builder.register(CustomLIF)\n",
    "def build_customlif(model, neuron_type, neurons):\n",
    "    \"\"\"Builds a `.LIF` object into a model.\n",
    "\n",
    "    In addition to adding a `.SimNeurons` operator, this build function sets up\n",
    "    signals to track the voltage and refractory times for each neuron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Model\n",
    "        The model to build into.\n",
    "    neuron_type : CustomLIF\n",
    "        Neuron type to build.\n",
    "    neuron : Neurons\n",
    "        The neuron population object corresponding to the neuron type.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Does not modify ``model.params[]`` and can therefore be called\n",
    "    more than once with the same `.LIF` instance.\n",
    "    \"\"\"\n",
    "\n",
    "    model.sig[neurons][\"voltage\"] = Signal(\n",
    "        np.zeros(neurons.size_in, dtype=rc.float_dtype),\n",
    "        name=\"%s.voltage\" % neurons\n",
    "    )\n",
    "    model.sig[neurons][\"threshold\"] = Signal(\n",
    "        np.ones(neurons.size_in, dtype=rc.float_dtype),\n",
    "        name= \"%s.threshold\" % neurons\n",
    "    )\n",
    "    model.add_op(\n",
    "        SimCustomLIF(\n",
    "            neurons=neuron_type,\n",
    "            J=model.sig[neurons][\"in\"],\n",
    "            output=model.sig[neurons][\"out\"],\n",
    "            states=[\n",
    "                model.sig[neurons][\"voltage\"],\n",
    "                model.sig[neurons][\"threshold\"]\n",
    "            ],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Constants\n",
    "nR = 1632      # Num Neurons in Retina\n",
    "nL = 512       # Num Neurons in LGN\n",
    "nV = 512       # Num Neurons in V1\n",
    "dt = 0.01\n",
    "runtime = 1.5\n",
    "steps = int(runtime / dt)\n",
    "\n",
    "# Retina Parameters\n",
    "ret_thresh = np.zeros((nR,))\n",
    "ret_v_reset = (0+0.1*(np.random.randn(nR,))**2) * 100 # Noise on activity field\n",
    "ret_fnoise = 3*np.random.randn(nR, steps)\n",
    "# Spatial Parameters\n",
    "sqR = 28\n",
    "ret_nx = sqR*np.random.rand(nR,2)\n",
    "ret_D = squareform(pdist(ret_nx))\n",
    "# Adjacency kernel\n",
    "ret_ri, ret_ro, ret_lam, ret_ai, ret_ao = 3, 6, 10, 30, 10\n",
    "ret_D = squareform(pdist(ret_nx))     # Non-dimensional distance matrix\n",
    "ret_S = ret_ai*(ret_D < ret_ri) - ret_ao*(ret_D > ret_ro) * np.exp(-ret_D / ret_lam); \n",
    "ret_S = ret_S - np.diag(np.diag(ret_S));    \n",
    "# Dynamics parameters\n",
    "ret_tau_v, ret_tau_th, ret_th_plus, ret_v_th = 1, 30, 9, 1\n",
    "ret_params = [ret_S, nR, ret_tau_v, ret_tau_th, ret_th_plus, ret_v_th, ret_v_reset, ret_nx, ret_fnoise, ret_thresh]\n",
    "\n",
    "\n",
    "# LGN Parameters\n",
    "LGN_thresh = np.random.normal(40,2,(nL,))\n",
    "LGN_v_reset = (0+0.1*(np.random.randn(nL,))**2) * 100 # Noise on activity field\n",
    "LGN_fnoise = 3*np.zeros((steps, nL))\n",
    "# Spatial Parameters\n",
    "sqL = 10\n",
    "LGN_nx = sqL*np.random.rand(nL,2)\n",
    "LGN_D = squareform(pdist(LGN_nx))\n",
    "# Adjacency kernel\n",
    "LGN_ri, LGN_ro, LGN_lam, LGN_ai, LGN_ao = 1.2, 1.5, 10, 30, 10\n",
    "LGN_D = squareform(pdist(LGN_nx))     # Non-dimensional distance matrix\n",
    "LGN_S = LGN_ai*(LGN_D < LGN_ri) - LGN_ao*(LGN_D > LGN_ro) * np.exp(-LGN_D / LGN_lam); \n",
    "LGN_S = LGN_S - np.diag(np.diag(LGN_S)); \n",
    "# Dynamics parameters\n",
    "LGN_tau_v, LGN_tau_th, LGN_th_plus, LGN_v_th = 1, 20, 12, 1\n",
    "LGN_params = [LGN_S, nL, LGN_tau_v, LGN_tau_th, LGN_th_plus, LGN_v_th, LGN_v_reset, LGN_nx, LGN_fnoise, LGN_thresh]\n",
    "\n",
    "# V1 Parameters\n",
    "V1_thresh = np.random.normal(40,2,(nV,))\n",
    "V1_v_reset = (0+0.1*(np.random.randn(nV,))**2) * 100 # Noise on activity field\n",
    "V1_fnoise = 3*np.zeros((steps, nV))\n",
    "# Spatial Parameters\n",
    "sqV = 10\n",
    "V1_nx = sqV*np.random.rand(nV,2)\n",
    "V1_D = squareform(pdist(LGN_nx))\n",
    "# Adjacency kernel\n",
    "V1_ri, V1_ro, V1_lam, V1_ai, V1_ao = 1.2, 1.5, 10, 30, 10\n",
    "V1_D = squareform(pdist(V1_nx))     # Non-dimensional distance matrix\n",
    "V1_S = V1_ai*(V1_D < V1_ri) - V1_ao*(V1_D > V1_ro) * np.exp(-V1_D / V1_lam); \n",
    "V1_S = V1_S - np.diag(np.diag(V1_S)); \n",
    "# Dynamics parameters\n",
    "V1_tau_v, V1_tau_th, V1_th_plus, V1_v_th = 1, 20, 12, 1\n",
    "V1_params = [V1_S, nV, V1_tau_v, V1_tau_th, V1_th_plus, V1_v_th, V1_v_reset, V1_nx, V1_fnoise, V1_thresh]\n",
    "\n",
    "\n",
    "# Synaptic Weight Matrix I (Ret-LGN1)\n",
    "mu_W1 = 2.5\n",
    "sigma_W1 = 0.14\n",
    "W1 = np.random.normal(mu_W1, sigma_W1, (nR,nL))\n",
    "W1 = W1/np.mean(W1, axis = 0) * mu_W1\n",
    "W1 = np.transpose(W1)\n",
    "\n",
    "# Synaptic Weight Matrix II (LGN1-V1)\n",
    "mu_W2 = 2.5\n",
    "sigma_W2 = 0.14\n",
    "W2 = np.random.normal(mu_W2, sigma_W2, (nL,nV))\n",
    "W2 = W2/np.mean(W2, axis = 0) * mu_W2 \n",
    "W2 = np.transpose(W2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vdom.v1+json": {
       "attributes": {},
       "tagName": "div"
      },
      "text/html": [
       "\n",
       "                <script>\n",
       "                    if (Jupyter.version.split(\".\")[0] < 5) {\n",
       "                        var pb = document.getElementById(\"01e6d914-44f9-4961-97b5-449109b933a5\");\n",
       "                        var text = document.createTextNode(\n",
       "                            \"HMTL progress bar requires Jupyter Notebook >= \" +\n",
       "                            \"5.0 or Jupyter Lab. Alternatively, you can use \" +\n",
       "                            \"TerminalProgressBar().\");\n",
       "                        pb.parentNode.insertBefore(text, pb);\n",
       "                    }\n",
       "                </script>\n",
       "                <div id=\"01e6d914-44f9-4961-97b5-449109b933a5\" style=\"\n",
       "                    width: 100%;\n",
       "                    border: 1px solid #cfcfcf;\n",
       "                    border-radius: 4px;\n",
       "                    text-align: center;\n",
       "                    position: relative;\">\n",
       "                  <div class=\"pb-text\" style=\"\n",
       "                      position: absolute;\n",
       "                      width: 100%;\">\n",
       "                    0%\n",
       "                  </div>\n",
       "                  <div class=\"pb-fill\" style=\"\n",
       "                      background-color: #bdd2e6;\n",
       "                      width: 0%;\">\n",
       "                    <style type=\"text/css\" scoped=\"scoped\">\n",
       "                        @keyframes pb-fill-anim {\n",
       "                            0% { background-position: 0 0; }\n",
       "                            100% { background-position: 100px 0; }\n",
       "                        }\n",
       "                    </style>\n",
       "                    &nbsp;\n",
       "                  </div>\n",
       "                </div>"
      ],
      "text/plain": [
       "HtmlProgressBar cannot be displayed. Please use the TerminalProgressBar. It can be enabled with `nengo.rc.set('progress', 'progress_bar', 'nengo.utils.progress.TerminalProgressBar')`."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vdom.v1+json": {
       "attributes": {
        "id": "f735cb6f-2b43-4e71-a2a8-3255d27e7fd4",
        "style": {
         "border": "1px solid #cfcfcf",
         "borderRadius": "4px",
         "boxSizing": "border-box",
         "position": "relative",
         "textAlign": "center",
         "width": "100%"
        }
       },
       "children": [
        {
         "attributes": {
          "class": "pb-text",
          "style": {
           "position": "absolute",
           "width": "100%"
          }
         },
         "children": [
          "Build finished in 0:00:01."
         ],
         "tagName": "div"
        },
        {
         "attributes": {
          "class": "pb-fill",
          "style": {
           "animation": "none",
           "backgroundColor": "#bdd2e6",
           "backgroundImage": "none",
           "backgroundSize": "100px 100%",
           "width": "100%"
          }
         },
         "children": [
          {
           "attributes": {
            "scoped": "scoped",
            "type": "text/css"
           },
           "children": [
            "\n                        @keyframes pb-fill-anim {\n                            0% { background-position: 0 0; }\n                            100% { background-position: 100px 0; }\n                        }}"
           ],
           "tagName": "style"
          },
          " "
         ],
         "tagName": "div"
        }
       ],
       "tagName": "div"
      },
      "text/html": [
       "<script>\n",
       "              (function () {\n",
       "                  var root = document.getElementById('01e6d914-44f9-4961-97b5-449109b933a5');\n",
       "                  var text = root.getElementsByClassName('pb-text')[0];\n",
       "                  var fill = root.getElementsByClassName('pb-fill')[0];\n",
       "\n",
       "                  text.innerHTML = 'Build finished in 0:00:01.';\n",
       "                  \n",
       "            fill.style.width = '100%';\n",
       "            fill.style.animation = 'pb-fill-anim 2s linear infinite';\n",
       "            fill.style.backgroundSize = '100px 100%';\n",
       "            fill.style.backgroundImage = 'repeating-linear-gradient(' +\n",
       "                '90deg, #bdd2e6, #edf2f8 40%, #bdd2e6 80%, #bdd2e6)';\n",
       "        \n",
       "                  \n",
       "                fill.style.animation = 'none';\n",
       "                fill.style.backgroundImage = 'none';\n",
       "            \n",
       "              })();\n",
       "        </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vdom.v1+json": {
       "attributes": {},
       "tagName": "div"
      },
      "text/html": [
       "\n",
       "                <script>\n",
       "                    if (Jupyter.version.split(\".\")[0] < 5) {\n",
       "                        var pb = document.getElementById(\"1336d643-00d4-4e27-85d4-9dba2e8a9bc5\");\n",
       "                        var text = document.createTextNode(\n",
       "                            \"HMTL progress bar requires Jupyter Notebook >= \" +\n",
       "                            \"5.0 or Jupyter Lab. Alternatively, you can use \" +\n",
       "                            \"TerminalProgressBar().\");\n",
       "                        pb.parentNode.insertBefore(text, pb);\n",
       "                    }\n",
       "                </script>\n",
       "                <div id=\"1336d643-00d4-4e27-85d4-9dba2e8a9bc5\" style=\"\n",
       "                    width: 100%;\n",
       "                    border: 1px solid #cfcfcf;\n",
       "                    border-radius: 4px;\n",
       "                    text-align: center;\n",
       "                    position: relative;\">\n",
       "                  <div class=\"pb-text\" style=\"\n",
       "                      position: absolute;\n",
       "                      width: 100%;\">\n",
       "                    0%\n",
       "                  </div>\n",
       "                  <div class=\"pb-fill\" style=\"\n",
       "                      background-color: #bdd2e6;\n",
       "                      width: 0%;\">\n",
       "                    <style type=\"text/css\" scoped=\"scoped\">\n",
       "                        @keyframes pb-fill-anim {\n",
       "                            0% { background-position: 0 0; }\n",
       "                            100% { background-position: 100px 0; }\n",
       "                        }\n",
       "                    </style>\n",
       "                    &nbsp;\n",
       "                  </div>\n",
       "                </div>"
      ],
      "text/plain": [
       "HtmlProgressBar cannot be displayed. Please use the TerminalProgressBar. It can be enabled with `nengo.rc.set('progress', 'progress_bar', 'nengo.utils.progress.TerminalProgressBar')`."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vdom.v1+json": {
       "attributes": {
        "id": "6fa67ac5-3e79-4b16-8604-9635ba3ee613",
        "style": {
         "border": "1px solid #cfcfcf",
         "borderRadius": "4px",
         "boxSizing": "border-box",
         "position": "relative",
         "textAlign": "center",
         "width": "100%"
        }
       },
       "children": [
        {
         "attributes": {
          "class": "pb-text",
          "style": {
           "position": "absolute",
           "width": "100%"
          }
         },
         "children": [
          "Simulation finished in 0:00:07."
         ],
         "tagName": "div"
        },
        {
         "attributes": {
          "class": "pb-fill",
          "style": {
           "animation": "none",
           "backgroundColor": "#bdd2e6",
           "backgroundImage": "none",
           "transition": "width 0.1s linear",
           "width": "100%"
          }
         },
         "children": [
          {
           "attributes": {
            "scoped": "scoped",
            "type": "text/css"
           },
           "children": [
            "\n                        @keyframes pb-fill-anim {\n                            0% { background-position: 0 0; }\n                            100% { background-position: 100px 0; }\n                        }}"
           ],
           "tagName": "style"
          },
          " "
         ],
         "tagName": "div"
        }
       ],
       "tagName": "div"
      },
      "text/html": [
       "<script>\n",
       "              (function () {\n",
       "                  var root = document.getElementById('1336d643-00d4-4e27-85d4-9dba2e8a9bc5');\n",
       "                  var text = root.getElementsByClassName('pb-text')[0];\n",
       "                  var fill = root.getElementsByClassName('pb-fill')[0];\n",
       "\n",
       "                  text.innerHTML = 'Simulation finished in 0:00:07.';\n",
       "                  \n",
       "            if (100.0 > 0.) {\n",
       "                fill.style.transition = 'width 0.1s linear';\n",
       "            } else {\n",
       "                fill.style.transition = 'none';\n",
       "            }\n",
       "\n",
       "            fill.style.width = '100.0%';\n",
       "            fill.style.animation = 'none';\n",
       "            fill.style.backgroundImage = 'none'\n",
       "        \n",
       "                  \n",
       "                fill.style.animation = 'none';\n",
       "                fill.style.backgroundImage = 'none';\n",
       "            \n",
       "              })();\n",
       "        </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nengo.utils.matplotlib import rasterplot\n",
    "# import nengo_loihi\n",
    "\n",
    "model = nengo.Network()\n",
    "with model:\n",
    "    ret = nengo.Ensemble(nR, dimensions=1, neuron_type=CustomLIF(*ret_params))\n",
    "    LGN = nengo.Ensemble(nL, dimensions=1, neuron_type=CustomLIF(*LGN_params))\n",
    "    V1 = nengo.Ensemble(nV, dimensions=1, neuron_type=CustomLIF(*V1_params))\n",
    "    \n",
    "    conn1 = nengo.Connection(ret.neurons, LGN.neurons, transform=W1, synapse = None)\n",
    "    conn1.learning_rule_type = STDP()\n",
    "    conn2 = nengo.Connection(LGN.neurons, V1.neurons, transform=W2, synapse = None)\n",
    "    conn2.learning_rule_type = STDP()\n",
    "\n",
    "    LR_pre = nengo.Probe(conn1.learning_rule, \"pre_filtered\")\n",
    "    LR_post = nengo.Probe(conn1.learning_rule, \"post_filtered\")\n",
    "    delta_probe = nengo.Probe(conn1.learning_rule, \"delta\")\n",
    "    weights_probe = nengo.Probe(conn1, \"weights\")\n",
    "\n",
    "    \n",
    "    ret_spikes = nengo.Probe(ret.neurons, 'spikes')\n",
    "    ret_voltage = nengo.Probe(ret.neurons, 'voltage')\n",
    "    ret_threshold = nengo.Probe(ret.neurons, 'threshold')\n",
    "\n",
    "    LGN_spikes = nengo.Probe(LGN.neurons, 'spikes')\n",
    "    LGN_voltage = nengo.Probe(LGN.neurons, 'voltage')\n",
    "    LGN_threshold = nengo.Probe(LGN.neurons, 'threshold')\n",
    "    \n",
    "    V1_spikes = nengo.Probe(V1.neurons, 'spikes')\n",
    "    V1_voltage = nengo.Probe(V1.neurons, 'voltage')\n",
    "    V1_threshold = nengo.Probe(V1.neurons, 'threshold')\n",
    "    \n",
    "    weights_p1 = nengo.Probe(conn1, 'weights', synapse=0.01, sample_every=0.01)\n",
    "    weights_p2 = nengo.Probe(conn2, 'weights', synapse=0.01, sample_every=0.01)\n",
    "\n",
    "\n",
    "\n",
    "with nengo.Simulator(model, dt = dt) as sim:\n",
    "# Initialize threshold (voltage and spiked default zero already)\n",
    "#     sim.signals[sim.model.sig[a.neurons]['threshold']] = np.ones((nR,))/ret_Vt\n",
    "#     sim.signals[sim.model.sig[b.neurons]['threshold']] = np.ones((nL,))/LGN_Vt\n",
    "\n",
    "    sim.run(runtime)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01, 0.01, ..., 0.04, 0.19, 0.19])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = sim.data[delta_probe]\n",
    "dp[dp!=0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.33900593, 2.33900593, 2.33900593, 2.33900593, 2.33900593,\n",
       "       2.33900593, 2.33900593, 2.33900593, 2.33900593, 2.33900593])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.data[weights_probe][40:50,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       ...,\n",
       "       [  0.,   0.,   0., ...,   0., 100.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0., 100.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0., 100.,   0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.data[LR_post]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how the weights are changing over time (as training happens)\n",
    "for i in range(80):\n",
    "    plt.title('W=' + str(i))\n",
    "    plt.imshow(sim.data[weights_probe][i])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.39190851e+00, 2.39190851e+00, 2.39190851e+00, 2.39190851e+00,\n",
       "       2.39190851e+00, 2.39190851e+00, 2.39190851e+00, 2.39190851e+00,\n",
       "       2.39190851e+00, 2.39190851e+00, 2.39190851e+00, 2.39190851e+00,\n",
       "       2.39190851e+00, 2.39190851e+00, 2.39190851e+00, 2.39190851e+00,\n",
       "       2.39190851e+00, 2.39190851e+00, 2.39190851e+00, 2.39190851e+00,\n",
       "       2.39190851e+00, 2.39190851e+00, 2.39190851e+00, 2.39190851e+00,\n",
       "       2.39190851e+00, 2.39190851e+00, 1.02391909e+02, 3.02391909e+02,\n",
       "       6.02391909e+02, 1.00239191e+03, 1.50239191e+03, 2.10239191e+03,\n",
       "       2.80239191e+03, 3.60239191e+03, 4.50239191e+03, 5.50239191e+03,\n",
       "       6.60239191e+03, 7.80239191e+03, 9.00239191e+03, 1.02023919e+04])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.data[weights_probe][0:40,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how finished ret-LGN weights (after 'training') pool (or not) to retina\n",
    "# each step is retina connections to a different LGN neuron\n",
    "W1end = sim.data[weights_probe][-1]\n",
    "for index in range(512):\n",
    "    #index = 10 # LGN neuron that we are taking W1 slice from\n",
    "    # connectivity from retina layer to LGN \"index\" neuron\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(131)\n",
    "    plt.scatter(ret_nx[:,0], ret_nx[:,1], c = W1end[index, :])\n",
    "    plt.title('Ret-Pools to LGN =' + str(index))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dynamics\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(sim.trange(), sim.data[LGN_threshold])\n",
    "# plt.xlabel('time [s]')\n",
    "# plt.ylabel('threshold')\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(sim.trange(), sim.data[V1_voltage])\n",
    "# plt.xlabel('time [s]')\n",
    "# plt.ylabel('voltage')\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# rasterplot(sim.trange(), sim.data[V1_spikes])\n",
    "# plt.xlabel('time [s]')\n",
    "# plt.ylabel('Neuron number')\n",
    "\n",
    "#plot voltage and threshold of one neuron on retina\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sim.trange(), sim.data[ret_voltage][:,100], label=\"voltage\")\n",
    "plt.plot(sim.trange(), sim.data[ret_threshold][:,100], label=\"threshold\")\n",
    "\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('voltage')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "rasterplot(sim.trange(), sim.data[ret_spikes][0:steps+1,100:101])\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('Neuron number')\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(sim.trange(), sim.data[ret_threshold][:,500])\n",
    "# plt.xlabel('time [s]')\n",
    "# plt.ylabel('threshold')\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(sim.trange(), sim.data[V1_voltage])\n",
    "# plt.xlabel('time [s]')\n",
    "# plt.ylabel('voltage')\n",
    "\n",
    "# #plot output weight changes of 500th neuron in layer 1 and 2, #respectively\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(sim.trange(sample_every=0.01), sim.data[weights_p1][..., 500])\n",
    "# plt.xlabel('time [s]')\n",
    "# plt.ylabel('Connection weight')\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(sim.trange(sample_every=0.01), sim.data[weights_p2][..., 500])\n",
    "# plt.xlabel('time [s]')\n",
    "# plt.ylabel('Connection weight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Wave\n",
    "for t in range(len(sim.trange())):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    fig.suptitle('Traveling wave for Ret, LGN, V1 layers')\n",
    "\n",
    "    ax1.scatter(ret_nx[:,0], ret_nx[:,1], color = 'k')\n",
    "    fired = np.argwhere(sim.data[ret_spikes][t,:])\n",
    "    ax1.scatter(ret_nx[fired,0], ret_nx[fired,1], color = 'r')\n",
    "    \n",
    "    ax2.scatter(LGN_nx[:,0], LGN_nx[:,1], color = 'k')\n",
    "    fired = np.argwhere(sim.data[LGN_spikes][t,:])\n",
    "    ax2.scatter(LGN_nx[fired,0], LGN_nx[fired,1], color = 'r')\n",
    "    \n",
    "    ax3.scatter(V1_nx[:,0], V1_nx[:,1], color = 'k')\n",
    "    fired = np.argwhere(sim.data[V1_spikes][t,:])\n",
    "    ax3.scatter(V1_nx[fired,0], V1_nx[fired,1], color = 'r')\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1632)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
